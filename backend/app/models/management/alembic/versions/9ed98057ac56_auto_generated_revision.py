"""Auto-generated revision

Revision ID: 9ed98057ac56
Revises: 
Create Date: 2024-11-29 17:35:10.531602

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '9ed98057ac56'
down_revision: Union[str, None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('datasets',
    sa.Column('_intid', sa.Integer(), nullable=False),
    sa.Column('id', sa.String(), sa.Computed("'DS' || _intid", ), nullable=False),
    sa.Column('name', sa.String(), nullable=False),
    sa.Column('description', sa.String(), nullable=True),
    sa.Column('file_path', sa.String(), nullable=False),
    sa.Column('uploaded_at', sa.DateTime(), nullable=False),
    sa.PrimaryKeyConstraint('_intid'),
    sa.UniqueConstraint('name')
    )
    with op.batch_alter_table('datasets', schema=None) as batch_op:
        batch_op.create_index(batch_op.f('ix_datasets_id'), ['id'], unique=False)

    op.create_table('eval_runs',
    sa.Column('_intid', sa.Integer(), nullable=False),
    sa.Column('id', sa.String(), sa.Computed("'ER' || _intid", ), nullable=False),
    sa.Column('eval_name', sa.String(), nullable=False),
    sa.Column('workflow_id', sa.String(), nullable=False),
    sa.Column('status', sa.Enum('PENDING', 'RUNNING', 'COMPLETED', 'FAILED', name='evalrunstatus'), nullable=False),
    sa.Column('output_variable', sa.String(), nullable=False),
    sa.Column('num_samples', sa.Integer(), nullable=False),
    sa.Column('start_time', sa.DateTime(), nullable=True),
    sa.Column('end_time', sa.DateTime(), nullable=True),
    sa.Column('results', sa.JSON(), nullable=True),
    sa.PrimaryKeyConstraint('_intid')
    )
    with op.batch_alter_table('eval_runs', schema=None) as batch_op:
        batch_op.create_index(batch_op.f('ix_eval_runs_id'), ['id'], unique=False)

    op.create_table('output_files',
    sa.Column('_intid', sa.Integer(), nullable=False),
    sa.Column('id', sa.String(), sa.Computed("'OF' || _intid", ), nullable=False),
    sa.Column('file_name', sa.String(), nullable=False),
    sa.Column('file_path', sa.String(), nullable=False),
    sa.Column('created_at', sa.DateTime(), nullable=False),
    sa.Column('updated_at', sa.DateTime(), nullable=False),
    sa.PrimaryKeyConstraint('_intid')
    )
    with op.batch_alter_table('output_files', schema=None) as batch_op:
        batch_op.create_index(batch_op.f('ix_output_files_id'), ['id'], unique=False)

    op.create_table('workflows',
    sa.Column('_intid', sa.Integer(), nullable=False),
    sa.Column('id', sa.String(), sa.Computed("'S' || _intid", ), nullable=False),
    sa.Column('name', sa.String(), nullable=False),
    sa.Column('description', sa.String(), nullable=True),
    sa.Column('definition', sa.JSON(), nullable=False),
    sa.Column('created_at', sa.DateTime(), nullable=False),
    sa.Column('updated_at', sa.DateTime(), nullable=False),
    sa.PrimaryKeyConstraint('_intid'),
    sa.UniqueConstraint('name')
    )
    with op.batch_alter_table('workflows', schema=None) as batch_op:
        batch_op.create_index(batch_op.f('ix_workflows_id'), ['id'], unique=False)

    op.create_table('runs',
    sa.Column('_intid', sa.Integer(), nullable=False),
    sa.Column('id', sa.String(), sa.Computed("'R' || _intid", ), nullable=False),
    sa.Column('workflow_id', sa.String(), nullable=False),
    sa.Column('parent_run_id', sa.String(), nullable=True),
    sa.Column('status', sa.Enum('PENDING', 'RUNNING', 'COMPLETED', 'FAILED', name='runstatus'), nullable=False),
    sa.Column('run_type', sa.String(), nullable=False),
    sa.Column('initial_inputs', sa.JSON(), nullable=True),
    sa.Column('input_dataset_id', sa.String(), nullable=True),
    sa.Column('start_time', sa.DateTime(), nullable=True),
    sa.Column('end_time', sa.DateTime(), nullable=True),
    sa.Column('outputs', sa.JSON(), nullable=True),
    sa.Column('output_file_id', sa.String(), nullable=True),
    sa.ForeignKeyConstraint(['input_dataset_id'], ['datasets.id'], ),
    sa.ForeignKeyConstraint(['output_file_id'], ['output_files.id'], ),
    sa.ForeignKeyConstraint(['parent_run_id'], ['runs.id'], ),
    sa.ForeignKeyConstraint(['workflow_id'], ['workflows.id'], ),
    sa.PrimaryKeyConstraint('_intid')
    )
    with op.batch_alter_table('runs', schema=None) as batch_op:
        batch_op.create_index(batch_op.f('ix_runs_id'), ['id'], unique=False)
        batch_op.create_index(batch_op.f('ix_runs_input_dataset_id'), ['input_dataset_id'], unique=False)
        batch_op.create_index(batch_op.f('ix_runs_parent_run_id'), ['parent_run_id'], unique=False)
        batch_op.create_index(batch_op.f('ix_runs_workflow_id'), ['workflow_id'], unique=False)

    op.create_table('tasks',
    sa.Column('_intid', sa.Integer(), nullable=False),
    sa.Column('id', sa.String(), sa.Computed("'T' || _intid", ), nullable=False),
    sa.Column('run_id', sa.String(), nullable=False),
    sa.Column('node_id', sa.String(), nullable=False),
    sa.Column('parent_task_id', sa.String(), nullable=True),
    sa.Column('status', sa.Enum('PENDING', 'RUNNING', 'COMPLETED', 'FAILED', name='taskstatus'), nullable=False),
    sa.Column('inputs', sa.JSON(), nullable=True),
    sa.Column('outputs', sa.JSON(), nullable=True),
    sa.Column('start_time', sa.DateTime(), nullable=True),
    sa.Column('end_time', sa.DateTime(), nullable=True),
    sa.ForeignKeyConstraint(['parent_task_id'], ['tasks.id'], ),
    sa.ForeignKeyConstraint(['run_id'], ['runs.id'], ),
    sa.PrimaryKeyConstraint('_intid')
    )
    with op.batch_alter_table('tasks', schema=None) as batch_op:
        batch_op.create_index(batch_op.f('ix_tasks_id'), ['id'], unique=False)

    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    with op.batch_alter_table('tasks', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('ix_tasks_id'))

    op.drop_table('tasks')
    with op.batch_alter_table('runs', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('ix_runs_workflow_id'))
        batch_op.drop_index(batch_op.f('ix_runs_parent_run_id'))
        batch_op.drop_index(batch_op.f('ix_runs_input_dataset_id'))
        batch_op.drop_index(batch_op.f('ix_runs_id'))

    op.drop_table('runs')
    with op.batch_alter_table('workflows', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('ix_workflows_id'))

    op.drop_table('workflows')
    with op.batch_alter_table('output_files', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('ix_output_files_id'))

    op.drop_table('output_files')
    with op.batch_alter_table('eval_runs', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('ix_eval_runs_id'))

    op.drop_table('eval_runs')
    with op.batch_alter_table('datasets', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('ix_datasets_id'))

    op.drop_table('datasets')
    # ### end Alembic commands ###
