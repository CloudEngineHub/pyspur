# ======================
# Core Configuration
# ======================

# Backend Configuration
BACKEND_PORT=8000
BACKEND_HOST=0.0.0.0
DEBUG=False

# Frontend Configuration
FRONTEND_PORT=3000
FRONTEND_HOST=0.0.0.0

# Nginx Configuration
# This is the port that will be used to access the application
NGINX_PORT=6080

# ======================
# Database Settings
# ======================
# leave these commented out if you wish to 
# use default sqlite database

# DB_NAME=your_db_name
# DB_USER=your_db_user
# DB_PASSWORD=your_db_password
# DB_HOST=localhost
# DB_PORT=5432


# ======================
# Model Provider API Keys
# ======================

# OPENAI_API_KEY=your_openai_api_key
# GEMINI_API_KEY=your_gemini_api_key
# ANTHROPIC_API_KEY=your_anthropic_api_key


# ======================
# Ollama Configuration
# ======================

# NOTE:
# if the ollama service is running on port 11434 of the host machine,
# then use http://host.docker.internal:11434 as the base url
# if the ollama service is running on a different host, use the ip address or domain name of the host

# Also make sure the ollama service is configured to accept requests. 
# This can be done setting OLLAMA_HOST=0.0.0.0 environment variable before launching the ollama service.

# OLLAMA_BASE_URL=http://host.docker.internal:11434 

# ======================