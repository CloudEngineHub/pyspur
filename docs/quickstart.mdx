---
title: 'Quickstart'
description: 'Get started with PySpur in under 2 minutes'
---

## Setup your development

Learn how to set up PySpur locally and start building powerful AI workflows.

### Installation Steps

<AccordionGroup>
  <Accordion icon="github" title="Clone the repository">
    ```sh
    git clone https://github.com/PySpur-com/pyspur.git
    cd pyspur
    ```
  </Accordion>
  <Accordion icon="gear" title="Configure environment">
    1. Create a `.env` file by copying the example:
    ```sh
    cp .env.example .env
    ```
    2. Review and update the configurations in `.env` as needed
    3. If you plan to use third-party model providers, add their API keys in the `.env` file
  </Accordion>
  <Accordion icon="docker" title="Launch PySpur">
    Start the docker services:
    ```sh
    docker compose -f ./docker-compose.prod.yml up --build -d
    ```
    Once complete, access PySpur at `http://localhost:6080/` in your browser
  </Accordion>
</AccordionGroup>

### Development Setup

<AccordionGroup>
<Accordion icon="code" title="Development Mode">
  For development purposes, use this command instead:
  ```sh
  docker compose up --build -d
  ```
  This will start PySpur in development mode with additional debugging features.
</Accordion>
</AccordionGroup>

### Managing API Keys

<AccordionGroup>
<Accordion icon="key" title="Configure API Keys">
  1. Access PySpur at `http://localhost:6080/`
  2. Click the settings icon (⚙️) in the top right corner
  3. Navigate to the API keys tab
  4. Enter your provider's API keys
  5. Click save to store your changes
</Accordion>
</AccordionGroup>

### Using Local Models with Ollama

<AccordionGroup>
<Accordion icon="server" title="Configure Ollama">
  1. Start Ollama service with:
  ```sh
  OLLAMA_HOST="0.0.0.0" ollama serve
  ```
  2. Update your `.env` file with:
  ```sh
  OLLAMA_BASE_URL=http://host.docker.internal:11434
  ```
  3. Download models using: `ollama pull <model-name>`
  4. Select Ollama models from the sidebar for LLM nodes

  Note: PySpur only works with models that support structured-output and json mode. Most newer models should be good, but please confirm this from Ollama documentation for the model you wish to use.
</Accordion>
</AccordionGroup>

## Next Steps

After installation, you can:

<CardGroup>
<Card title="Create New Workflow" icon="wand-magic-sparkles">
  Click "New Spur" to create a workflow from scratch
</Card>

<Card title="Use Templates" icon="copy">
  Start with one of our pre-built templates
</Card>
</CardGroup>

## Need Help?

<CardGroup>
<Card
  title="Join Our Discord"
  icon="discord"
  href="https://discord.gg/7Spn7C8A5F"
>
  Connect with the community and get help
</Card>

<Card
  title="Talk to Creators"
  icon="calendar"
  href="https://calendly.com/d/cnf9-57m-bv3/pyspur-founders"
>
  Schedule a call with the PySpur team
</Card>
</CardGroup>
